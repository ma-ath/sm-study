{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Moments about the Origin (Peebles chapter 5, page 143)\n",
    "\n",
    "Let $X$, $Y$ be random variables and $f_{XY}$ be their joint density function. Joint moment $m_{nk}$ is defined by\n",
    "\n",
    "$$\n",
    "m_{nk} = E[X^n Y^k] = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} x^{n}y^{k} f_{XY} (x,y) dx dy\n",
    "$$\n",
    "\n",
    "The sum $n+k$ is called the *order* of the moment. The second-order moment $m_{11}$ has a special name and is called the *correlation* of random variables $X$ and $Y$. It is denoted by the symbol\n",
    "\n",
    "$$\n",
    "m_{11}=R_{XY}\n",
    "$$\n",
    "\n",
    "In case where $E[XY] = E[X]E[Y]$, $X$ and $Y$ are called to be *uncorrelated*. In case $R_{XY}=0$, $X$ and $Y$ are called *ortogonal*.\n",
    "\n",
    "In the discrete case where the real pair of random variables $(X,Y)$ have $m$ realizations $(x_i,y_i)$, $0 \\leq i \\leq m$, with have equal probabilities, that is, $f_{XY}(x,y) = \\frac{1}{m^2}$, integral equation becomes simply:\n",
    "\n",
    "$$\n",
    "m_{nk} = E[X^n Y^k] = \\frac{1}{m}\\sum_{i=0}^{m} x_i^{n}y_i^{k}\n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\n",
    "R_{XY} = \\frac{1}{m}\\sum_{i=0}^{m} x_i y_i\n",
    "$$\n",
    "\n",
    "**NOTE:**\n",
    "$$\n",
    "\\sum_{i=0}^{m} \\sum_{j=0}^{m} \\frac{1}{m^2} x_i y_i = \\frac{1}{m^2} \\sum_{i=0}^{m} \\sum_{j=0}^{m} x_i y_i = \\frac{1}{m^2} \\left[ \\sum_{i=0}^{m} x_i y_i + \\sum_{i=0}^{m} x_i y_i + \\ldots + \\sum_{i=0}^{m} x_i y_i \\right] = \\frac{m}{m^2} \\sum_{i=0}^{m} x_i y_i = \\frac{1}{m} \\sum_{i=0}^{m} x_i y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Central Moments (Peebles chapter 5, page 144)\n",
    "\n",
    "Joint central moment $\\mu_{nk}$ is defined as:\n",
    "\n",
    "$$\\mu_{nk}=E[(X-\\bar{X})^n(Y-\\bar{Y})^k] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (x-\\bar{X})^{n}(y-\\bar{Y})^{k} f_{XY} (x,y) dx dy$$\n",
    "\n",
    "Second order moments $\\mu_{20}=E[(X-\\bar{X})^2]$ and $\\mu_{02}=E[(Y-\\bar{Y})^2]$ are simply the variances $\\sigma_X^2$ and $\\sigma_Y^2$ of $X$ and $Y$.\n",
    "\n",
    "The second order moment $\\mu_{11}$ is very important and is called the *covariance* of $X$ nad $Y$. It is depicted by\n",
    "\n",
    "$$\\mu_{11}=C_{XY}=E[(X-\\bar{X})(Y-\\bar{Y})]$$\n",
    "\n",
    "By expandind the integral, one can see that it reduces to:\n",
    "\n",
    "$$C_{XY}=R_{XY} - \\bar{X}\\bar{Y}$$\n",
    "\n",
    "and therefore, $C_{XY}=0$ if $X$ and $Y$ are either *independent* or *uncorrelated* ($R_{XY}=\\bar{X}\\bar{Y}$).\n",
    "\n",
    "If $X$ and $Y$ are ortogonoal, $R_{XY}=0$ and $C_{XY}=-\\bar{X}\\bar{Y}$.\n",
    "\n",
    "The normalized second-order moment\n",
    "$$\\rho_{XY}=\\mu_{11}/\\sqrt{\\mu_{20}\\mu_{02}} = C_{XY}/\\sigma_{X}\\sigma_{Y}$$\n",
    "\n",
    "is known as *correlation coefficient* and $-1<\\rho<1$\n",
    "\n",
    "In the discrete case where the real pair of random variables $(X,Y)$ have $m$ realizations $(x_i,y_i)$, $0 \\leq i \\leq m$, with have equal probabilities, that is, $f_{XY}(x,y) = \\frac{1}{m^2}$, integral equation becomes simply:\n",
    "\n",
    "$$\n",
    "\\mu_{nk}=E[(X-\\bar{X})^n(Y-\\bar{Y})^k] = \\frac{1}{m} \\sum_{i=0}^{m} (x_i-\\bar{X})^{n}(y_i-\\bar{Y})^{k}\n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\n",
    "C_{XY}=E[(X-\\bar{X})(Y-\\bar{Y})] = \\frac{1}{m} \\sum_{i=0}^{m} (x_i-\\bar{X})(y_i-\\bar{Y})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear combination of random variables, covariance and correlation Matrices\n",
    "\n",
    "Let $X=[X_1 \\dots X_n]^T$ be a vector of $n$ real-valued random variables.\n",
    "\n",
    "Assuming that the covariances $C_{X_i X_j}, 0<ij<n$ are well defined, the *covariance matrix* $\\textbf{C}$ is defined as:\n",
    "\n",
    "$$\\textbf{C} = \\begin{bmatrix}\n",
    "C_{X_1 X_1} & C_{X_1 X_2} & \\ldots & C_{X_1 X_n}\\\\\n",
    "C_{X_2 X_1} & C_{X_2 X_2} & \\ldots & C_{X_2 X_n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "C_{X_n X_1} & C_{X_n X_2} & \\ldots & C_{X_n X_n}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The covariance matrix together with the vector of means $\\textbf{m}=[\\bar{X_1} \\bar{X_2} \\ldots \\bar{X_n}]^T$ allows to calculate first two moments of any linear combination of the $n$ random variables:\n",
    "\n",
    "$$Y=a_1 X_1 + a_2 X_2 + \\ldots + a_n X_n \\\\\n",
    "\\rightarrow \\bar{Y} = \\textbf{a}^T \\textbf{m} \\\\\n",
    "\\rightarrow \\sigma_{Y}^2=E[(Y-\\bar{Y})^2] = \\ldots = \\textbf{a}^T \\textbf{C} \\textbf{a}$$\n",
    "\n",
    "The *correlation matrix* is the matrix of correlation coefficients $\\rho_{X_i X_j}$ and is defined as:\n",
    "\n",
    "$$\\textbf{R} = \\begin{bmatrix}\n",
    "\\rho_{X_1 X_1} & \\rho_{X_1 X_2} & \\ldots & \\rho_{X_1 X_n}\\\\\n",
    "\\rho_{X_2 X_1} & \\rho_{X_2 X_2} & \\ldots & \\rho_{X_2 X_n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\rho_{X_n X_1} & \\rho_{X_n X_2} & \\ldots & \\rho_{X_n X_n}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "if $\\textbf{r}=[a_1\\sigma_1,a_2\\sigma_2,\\ldots,a_n\\sigma_n]^T$, therefore:\n",
    "\n",
    "$$\\sigma_Y^2=\\textbf{r}^T\\textbf{R}\\textbf{r}$$\n",
    "\n",
    "In case where variable $X_1 \\ldots X_n$ are independent, the covariance and correlation matrices are simply:\n",
    "\n",
    "$$\\textbf{C} = \\begin{bmatrix}\n",
    "\\sigma_{X_1}^2 & 0 & \\ldots & 0\\\\\n",
    "0 & \\sigma_{X_2}^2 & \\ldots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & \\ldots & \\sigma_{X_n}^2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\textbf{R} = \\begin{bmatrix}\n",
    "1 & 0 & \\ldots & 0\\\\\n",
    "0 & 1 & \\ldots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & \\ldots & 1\\\\\n",
    "\\end{bmatrix} = \\textbf{I}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Part 1 - Correlation Matrix, SVD and Eigenvalues\n",
    "\n",
    "let $\\textbf{X}_{m\\times n}$ be data matrix of rank $r \\leq \\min\\{m,n\\}$ comprising by $m$ observetions of $n$ random variables\n",
    "\n",
    "$$\\textbf{X} = \\begin{bmatrix}\n",
    "\\vert & \\vert & \\ldots & \\vert \\\\\n",
    "\\textbf{x}_1 & \\textbf{x}_2 & \\ldots & \\textbf{x}_n\\\\\n",
    "\\vert & \\vert & \\ldots & \\vert \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Matrix $\\textbf{X}$ has an SVD decomposition\n",
    "\n",
    "$$ \\textbf{X} =\\textbf{U}\\Sigma \\textbf{V}^T \\text{,}$$\n",
    "\n",
    "where columns of $\\textbf{U}_{m \\times m}$ are eigenvectors of $\\textbf{X}\\textbf{X}^T$, columns of $\\textbf{V}_{n \\times n}$ are eigenvectors of $\\textbf{X}^T\\textbf{X}$, and $\\Sigma$ is a diagonal matrix of the square-roots of the $r$ non-zero singular values of $\\textbf{X}\\textbf{X}^T$ and $\\textbf{X}^T\\textbf{X}$\n",
    "\n",
    "Matrix $\\textbf{X}$ is usually processed so that column vectors $x_i$ have zero mean, that is, $x_i^T \\textbf{1} = \\textbf{0}$. If we also divide the zero-mean vectors $x_i$ by $\\sqrt{m}$, matrix $\\textbf{X}$ becomes\n",
    "\n",
    "$$\\textbf{X} = \\begin{bmatrix}\n",
    "\\vert & \\vert & \\ldots & \\vert \\\\\n",
    "\\frac{\\textbf{x}_1}{\\sqrt{m}} & \\frac{\\textbf{x}_2}{\\sqrt{m}} & \\ldots & \\frac{\\textbf{x}_n}{\\sqrt{m}}\\\\\n",
    "\\vert & \\vert & \\ldots & \\vert \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "And matrix $\\textbf{X}^T\\textbf{X}$ becomes:\n",
    "\n",
    "$$\n",
    "\\textbf{X}^T\\textbf{X}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\text{---} & \\frac{\\textbf{x}_1}{\\sqrt{m}} & \\text{---} \\\\\n",
    "\\text{---} & \\frac{\\textbf{x}_2}{\\sqrt{m}} & \\text{---} \\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "\\text{---} & \\frac{\\textbf{x}_n}{\\sqrt{m}} & \\text{---} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\vert & \\vert & \\ldots & \\vert \\\\\n",
    "\\frac{\\textbf{x}_1}{\\sqrt{m}} & \\frac{\\textbf{x}_2}{\\sqrt{m}} & \\ldots & \\frac{\\textbf{x}_n}{\\sqrt{m}}\\\\\n",
    "\\vert & \\vert & \\ldots & \\vert \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\textbf{x}_1^T \\textbf{x}_1}{m} & \\frac{\\textbf{x}_1^T \\textbf{x}_2}{m} & \\ldots & \\frac{\\textbf{x}_1^T \\textbf{x}_n}{m} \\\\\n",
    "\\frac{\\textbf{x}_2^T \\textbf{x}_1}{m} & \\frac{\\textbf{x}_2^T \\textbf{x}_2}{m} & \\ldots & \\frac{\\textbf{x}_2^T \\textbf{x}_n}{m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\textbf{x}_n^T \\textbf{x}_1}{m} & \\frac{\\textbf{x}_n^T \\textbf{x}_2}{m} & \\ldots & \\frac{\\textbf{x}_n^T \\textbf{x}_n}{m} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Since\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\textbf{x}_{i}^T \\textbf{x}_{j} = \\frac{1}{m} \\sum_{k=0}^m x_{i}[k] x_{j}[k] = R_{X_i X_j}\n",
    "$$\n",
    "\n",
    "for random processes $X_i$, $X_j$ of constant joint density function, that is, realizations have equal probabilities, and since we processed $x_i$ and $x_j$ to have zero mean, that is\n",
    "\n",
    "$$\n",
    "\\bar{X}_i = 0, \\bar{X}_j = 0\n",
    "$$\n",
    "\n",
    "we have that\n",
    "\n",
    "$$\n",
    "C_{X_i X_j} = R_{X_i X_j}\n",
    "$$\n",
    "\n",
    "and therefore, matrix $\\textbf{X}^T\\textbf{X}$ is the covariant matrix $\\textbf{C}$ of random variables. The eigenvectors of $\\textbf{X}^T\\textbf{X}$ are such that, for any vector $\\textbf{v}$,\n",
    "\n",
    "$$\n",
    "\\textbf{X}^T\\textbf{X} \\textbf{v} = \\lambda \\textbf{v} \\rightarrow |\\textbf{X}^T\\textbf{X} - \\lambda \\textbf{I}| = 0\n",
    "$$\n",
    "\n",
    "Intuitively, eigenvector $\\textbf{v}_i$ associated with the largest eigenvalue $\\lambda_i$ is the direction of maximum variance of the data matrix. Since matrix $\\Sigma$ is composed of the eigenvalues of the covariance matrix $\\textbf{X}^T\\textbf{X}$, one can use the $k$ first singular-values of $\\Sigma$ to construct an optimal approximation of $\\textbf{X}$ that preseves its maximum variance directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Example\n",
    "\n",
    "Data matrix\n",
    "$\\textbf{X} = \\begin{bmatrix}\n",
    "\\vert & \\vert & \\vert & \\vert \\\\\n",
    "\\textbf{x}_1 & \\textbf{x}_2 & \\textbf{x}_3 & \\textbf{x}_4\\\\\n",
    "\\vert & \\vert & \\vert & \\vert \\\\\n",
    "\\end{bmatrix}$ has 20 observations ($X_{20 \\times 4}$) of 2 uncorrelated random variables ($x_1$, $x_2$), and 2 correlated random variables ($x_3$, $x_4$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[ 20.4473,   2.0866,  38.8080, -14.1875],\n",
      "        [-11.2823,  -8.3306, -14.2340, -13.7095],\n",
      "        [ 14.3038, -18.3297,  46.9373, -69.2928],\n",
      "        [-23.1661,  27.1648, -73.4970, 104.6605],\n",
      "        [ -8.6889, -28.0377,  10.6599, -75.4241],\n",
      "        [ -8.3530,  -9.0492,  -7.6568, -18.7946],\n",
      "        [-12.6888, -14.1977, -11.1798, -29.9044],\n",
      "        [ 19.0521,  38.6414,  -0.5371,  96.8719],\n",
      "        [ -2.4533,  -2.7982,  -2.1084,  -5.9414],\n",
      "        [ 11.6427, -13.1820,  36.4673, -51.1886],\n",
      "        [ -3.9992,  10.2348, -18.2333,  34.7038],\n",
      "        [ 10.6465,  18.5258,   2.7671,  44.9309],\n",
      "        [ -4.1662, -12.7951,   4.4626, -34.2189],\n",
      "        [ -6.2387,  30.8977, -43.3751,  98.9318],\n",
      "        [ 20.8104,  24.9851,  16.6357,  54.1448],\n",
      "        [-10.3206, -19.8051,  -0.8360, -49.0949],\n",
      "        [ -3.4714, -17.1400,  10.1972, -47.9487],\n",
      "        [  2.6619,   4.2073,   1.1165,   9.9600],\n",
      "        [ -9.0755,  -2.6394, -15.5116,   1.1574],\n",
      "        [  4.3393, -10.4388,  19.1174, -35.6558]]) \n",
      "\n",
      "XtX: tensor([[  2943.0117,   1124.6705,   4761.3525,    431.0005],\n",
      "        [  1124.6705,   6902.2363,  -4652.8940,  19582.0371],\n",
      "        [  4761.3525,  -4652.8940,  14175.6025, -18720.0352],\n",
      "        [   431.0005,  19582.0371, -18720.0352,  58315.1094]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "n_observations = 20\n",
    "\n",
    "# Original values - The more observations we have, the more we can expect random variables x1 and x2 to be uncorelated.\n",
    "x1=200*(torch.rand(n_observations)-0.5)     # variable x1 is uniformely distributed between [-100, 100]\n",
    "x2=300*(torch.rand(n_observations))     # variable x2 is uniformely distributed between [0, 300]\n",
    "# Variables x3 and x4 are linear combinations of x1 and x2\n",
    "x3= 2*x1-x2\n",
    "x4= 3*x2-x1\n",
    "\n",
    "# Normalize mean\n",
    "x1=x1-x1.mean()\n",
    "x2=x2-x2.mean()\n",
    "x3=x3-x3.mean()\n",
    "x4=x4-x4.mean()\n",
    "\n",
    "# Divide by sqrt(n_observations)\n",
    "x1=torch.div(x1, math.sqrt(n_observations))\n",
    "x2=torch.div(x2, math.sqrt(n_observations))\n",
    "x3=torch.div(x3, math.sqrt(n_observations))\n",
    "x4=torch.div(x4, math.sqrt(n_observations))\n",
    "\n",
    "# Data matrix X\n",
    "X=torch.stack((x1,x2,x3,x4),dim=1)\n",
    "\n",
    "print(\"X:\", X, \"\\n\")\n",
    "\n",
    "# Covariance matrix X^T*X\n",
    "XtX = torch.matmul(torch.transpose(X, 0,1),X)\n",
    "\n",
    "print(\"XtX:\", XtX, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S: tensor([2.6720e+02, 1.0459e+02, 8.6716e-06, 8.2054e-06]) \n",
      "\n",
      "V: tensor([[ 0.0117, -0.2961,  0.3195, -0.9001],\n",
      "        [ 0.5178,  0.2422,  0.7935,  0.2087],\n",
      "        [-0.7687,  0.5285,  0.3556, -0.0576],\n",
      "        [ 0.3752,  0.7579, -0.3767, -0.3782]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate SVD\n",
    "U,S,V = torch.linalg.svd(X)\n",
    "\n",
    "#print(\"U:\",U,\"\\n\") #Too big to display correctly\n",
    "print(\"S:\",S,\"\\n\")\n",
    "print(\"V:\",V,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the SVD, it is clear that singular values $\\sigma_3$ and $\\sigma_4$ are not significant to the data matrix. This is expected, since we can find variables ($x_3$, $x_4$) as a linear combination of ($x_1$, $x_2$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Part 2 - Finding Components\n",
    "\n",
    "*\"[...] In order to achieve these goals, PCA computes new variables called principal components which are obtained as linear combinations of the original variables. The first principal component is required to have the largest possible variance (i.e., inertia and therefore this component will ‘explain’ or ‘extract’ the largest part of the inertia of the data table). The second component is computed under the\n",
    "constraint of being orthogonal to the first component and to have the largest possible inertia. The other components are computed likewise (see Appendix A for proof). The values of these new variables for the observations are called factor scores, and these factors scores can be interpreted geometrically as the projections of the observations onto the principal components.\"*\n",
    "\n",
    "With the SVD matrix $\\textbf{X} = \\textbf{U}\\Sigma \\textbf{V}^T$, we define a matrix of *factor scores* $\\textbf{F}$ as:\n",
    "\n",
    "$$\n",
    "\\textbf{F} = \\textbf{U} \\Sigma\n",
    "$$\n",
    "\n",
    "*Factor scores* are the projections of the observations onto the principal components.\n",
    "\n",
    "Matrix $\\textbf{V}$ is a projection matrix of $\\textbf{X}$ into the factor scores, as\n",
    "\n",
    "$$\n",
    "\\textbf{X}\\textbf{V} = \\textbf{U}\\Sigma\\textbf{V}^T\\textbf{V} = \\textbf{U}\\Sigma = \\textbf{F}\n",
    "$$\n",
    "\n",
    "We can also project a vector $\\textbf{a}^T$, pre-processed the same way as matrix $\\textbf{X}$, onto the principal components as\n",
    "\n",
    "$$\n",
    "\\textbf{a}_F^T = \\textbf{a}^T \\textbf{Q}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "*Probability, Random Variables and Random Signal Proncipals - Peyton Peebles* (ASIN:0070474281;ISBN-10:9780070474284;ISBN-13:978-0070474284)\n",
    "\n",
    "*Covariance and correlation matrix. Sums of random variables, their distributions, characteristics and characteristic functions.*\n",
    "https://web.sgh.waw.pl/~rlocho/classes_5_PTSP2019.pdf\n",
    "\n",
    "*Principal component analysis - Herve Abdi and Lynne J. Williams* (https://wires.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/wics.101?download=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "016cd2178a8053b5595958e59a0d24370f913bb5fc4da5760f6ef0caa289bc0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
