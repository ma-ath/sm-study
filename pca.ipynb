{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Moments about the Origin (Peebles chapter 5, page 143)\n",
    "\n",
    "Let $X$, $Y$ be random variables and $f_{XY}$ be their joint density function. Joint moment $m_{nk}$ is defined by\n",
    "\n",
    "$$\n",
    "m_{nk} = E[X^n Y^k] = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} x^{n}y^{k} f_{XY} (x,y) dx dy\n",
    "$$\n",
    "\n",
    "The sum $n+k$ is called the *order* of the moment. The second-order moment $m_{11}$ has a special name and is called the *correlation* of random variables $X$ and $Y$. It is denoted by the symbol\n",
    "\n",
    "$$\n",
    "m_{11}=R_{XY}\n",
    "$$\n",
    "\n",
    "In case where $E[XY] = E[X]E[Y]$, $X$ and $Y$ are called to be *uncorrelated*. In case $R_{XY}=0$, $X$ and $Y$ are called *ortogonal*.\n",
    "\n",
    "In the discrete case where the real pair of random variables $(X,Y)$ have $m$ realizations $(x_i,y_i)$, $0 \\leq i \\leq m$, with have equal probabilities, that is, $f_{XY}(x,y) = \\frac{1}{m^2}$, integral equation becomes simply:\n",
    "\n",
    "$$\n",
    "m_{nk} = E[X^n Y^k] = \\frac{1}{m}\\sum_{i=0}^{m} x_i^{n}y_i^{k}\n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\n",
    "R_{XY} = \\frac{1}{m}\\sum_{i=0}^{m} x_i y_i\n",
    "$$\n",
    "\n",
    "**NOTE:**\n",
    "$$\n",
    "\\sum_{i=0}^{m} \\sum_{j=0}^{m} \\frac{1}{m^2} x_i y_i = \\frac{1}{m^2} \\sum_{i=0}^{m} \\sum_{j=0}^{m} x_i y_i = \\frac{1}{m^2} \\left[ \\sum_{i=0}^{m} x_i y_i + \\sum_{i=0}^{m} x_i y_i + \\ldots + \\sum_{i=0}^{m} x_i y_i \\right] = \\frac{m}{m^2} \\sum_{i=0}^{m} x_i y_i = \\frac{1}{m} \\sum_{i=0}^{m} x_i y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Central Moments (Peebles chapter 5, page 144)\n",
    "\n",
    "Joint central moment $\\mu_{nk}$ is defined as:\n",
    "\n",
    "$$\\mu_{nk}=E[(X-\\bar{X})^n(Y-\\bar{Y})^k] = \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} (x-\\bar{X})^{n}(y-\\bar{Y})^{k} f_{XY} (x,y) dx dy$$\n",
    "\n",
    "Second order moments $\\mu_{20}=E[(X-\\bar{X})^2]$ and $\\mu_{02}=E[(Y-\\bar{Y})^2]$ are simply the variances $\\sigma_X^2$ and $\\sigma_Y^2$ of $X$ and $Y$.\n",
    "\n",
    "The second order moment $\\mu_{11}$ is very important and is called the *covariance* of $X$ nad $Y$. It is depicted by\n",
    "\n",
    "$$\\mu_{11}=C_{XY}=E[(X-\\bar{X})(Y-\\bar{Y})]$$\n",
    "\n",
    "By expandind the integral, one can see that it reduces to:\n",
    "\n",
    "$$C_{XY}=R_{XY} - \\bar{X}\\bar{Y}$$\n",
    "\n",
    "and therefore, $C_{XY}=0$ if $X$ and $Y$ are either *independent* or *uncorrelated* ($R_{XY}=\\bar{X}\\bar{Y}$).\n",
    "\n",
    "If $X$ and $Y$ are ortogonoal, $R_{XY}=0$ and $C_{XY}=-\\bar{X}\\bar{Y}$.\n",
    "\n",
    "The normalized second-order moment\n",
    "$$\\rho_{XY}=\\mu_{11}/\\sqrt{\\mu_{20}\\mu_{02}} = C_{XY}/\\sigma_{X}\\sigma_{Y}$$\n",
    "\n",
    "is known as *correlation coefficient* and $-1<\\rho<1$\n",
    "\n",
    "In the discrete case where the real pair of random variables $(X,Y)$ have $m$ realizations $(x_i,y_i)$, $0 \\leq i \\leq m$, with have equal probabilities, that is, $f_{XY}(x,y) = \\frac{1}{m^2}$, integral equation becomes simply:\n",
    "\n",
    "$$\n",
    "\\mu_{nk}=E[(X-\\bar{X})^n(Y-\\bar{Y})^k] = \\frac{1}{m} \\sum_{i=0}^{m} (x_i-\\bar{X})^{n}(y_i-\\bar{Y})^{k}\n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\n",
    "C_{XY}=E[(X-\\bar{X})(Y-\\bar{Y})] = \\frac{1}{m} \\sum_{i=0}^{m} (x_i-\\bar{X})(y_i-\\bar{Y})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear combination of random variables, covariance and correlation Matrices\n",
    "\n",
    "Let $X=[X_1 \\dots X_n]^T$ be a vector of $n$ real-valued random variables.\n",
    "\n",
    "Assuming that the covariances $C_{X_i X_j}, 0<ij<n$ are well defined, the *covariance matrix* $\\textbf{C}$ is defined as:\n",
    "\n",
    "$$\\textbf{C} = \\begin{bmatrix}\n",
    "C_{X_1 X_1} & C_{X_1 X_2} & \\ldots & C_{X_1 X_n}\\\\\n",
    "C_{X_2 X_1} & C_{X_2 X_2} & \\ldots & C_{X_2 X_n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "C_{X_n X_1} & C_{X_n X_2} & \\ldots & C_{X_n X_n}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The covariance matrix together with the vector of means $\\textbf{m}=[\\bar{X_1} \\bar{X_2} \\ldots \\bar{X_n}]^T$ allows to calculate first two moments of any linear combination of the $n$ random variables:\n",
    "\n",
    "$$Y=a_1 X_1 + a_2 X_2 + \\ldots + a_n X_n \\\\\n",
    "\\rightarrow \\bar{Y} = \\textbf{a}^T \\textbf{m} \\\\\n",
    "\\rightarrow \\sigma_{Y}^2=E[(Y-\\bar{Y})^2] = \\ldots = \\textbf{a}^T \\textbf{C} \\textbf{a}$$\n",
    "\n",
    "The *correlation matrix* is the matrix of correlation coefficients $\\rho_{X_i X_j}$ and is defined as:\n",
    "\n",
    "$$\\textbf{R} = \\begin{bmatrix}\n",
    "\\rho_{X_1 X_1} & \\rho_{X_1 X_2} & \\ldots & \\rho_{X_1 X_n}\\\\\n",
    "\\rho_{X_2 X_1} & \\rho_{X_2 X_2} & \\ldots & \\rho_{X_2 X_n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\rho_{X_n X_1} & \\rho_{X_n X_2} & \\ldots & \\rho_{X_n X_n}\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "if $\\textbf{r}=[a_1\\sigma_1,a_2\\sigma_2,\\ldots,a_n\\sigma_n]^T$, therefore:\n",
    "\n",
    "$$\\sigma_Y^2=\\textbf{r}^T\\textbf{R}\\textbf{r}$$\n",
    "\n",
    "In case where variable $X_1 \\ldots X_n$ are independent, the covariance and correlation matrices are simply:\n",
    "\n",
    "$$\\textbf{C} = \\begin{bmatrix}\n",
    "\\sigma_{X_1}^2 & 0 & \\ldots & 0\\\\\n",
    "0 & \\sigma_{X_2}^2 & \\ldots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & \\ldots & \\sigma_{X_n}^2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\textbf{R} = \\begin{bmatrix}\n",
    "1 & 0 & \\ldots & 0\\\\\n",
    "0 & 1 & \\ldots & 0\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & \\ldots & 1\\\\\n",
    "\\end{bmatrix} = \\textbf{I}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Part 1 - SVD, Correlation Matrix and Eigenvalues\n",
    "\n",
    "let $\\textbf{X}_{m\\times n}$ be data matrix of rank $r \\leq \\min\\{m,n\\}$ comprising by $m$ observetions of $n$ random variables\n",
    "\n",
    "$$\\textbf{X} = \\begin{bmatrix}\n",
    "\\vert & \\vert & \\ldots & \\vert \\\\\n",
    "\\textbf{x}_1 & \\textbf{x}_2 & \\ldots & \\textbf{x}_n\\\\\n",
    "\\vert & \\vert & \\ldots & \\vert \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Matrix $\\textbf{X}$ has an SVD decomposition\n",
    "\n",
    "$$ \\textbf{X} =\\textbf{U}\\Sigma \\textbf{V}^T \\text{,}$$\n",
    "\n",
    "where columns of $\\textbf{U}_{m \\times m}$ are eigenvectors of $\\textbf{X}\\textbf{X}^T$, columns of $\\textbf{V}_{n \\times n}$ are eigenvectors of $\\textbf{X}^T\\textbf{X}$, and $\\Sigma$ is a diagonal matrix of the square-roots of the $r$ non-zero singular values of $\\textbf{X}\\textbf{X}^T$ and $\\textbf{X}^T\\textbf{X}$\n",
    "\n",
    "Matrix $\\textbf{X}$ is usually processed so that column vectors $x_i$ have zero mean, that is, $x_i^T \\textbf{1} = \\textbf{0}$. If we also divide the zero-mean vectors $x_i$ by $\\sqrt{m}$, matrix $\\textbf{X}$ becomes\n",
    "\n",
    "$$\\textbf{X} = \\begin{bmatrix}\n",
    "\\vert & \\vert & \\ldots & \\vert \\\\\n",
    "\\frac{\\textbf{x}_1}{\\sqrt{m}} & \\frac{\\textbf{x}_2}{\\sqrt{m}} & \\ldots & \\frac{\\textbf{x}_n}{\\sqrt{m}}\\\\\n",
    "\\vert & \\vert & \\ldots & \\vert \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "And matrix $\\textbf{X}^T\\textbf{X}$ becomes:\n",
    "\n",
    "$$\n",
    "\\textbf{X}^T\\textbf{X}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\text{---} & \\frac{\\textbf{x}_1}{\\sqrt{m}} & \\text{---} \\\\\n",
    "\\text{---} & \\frac{\\textbf{x}_2}{\\sqrt{m}} & \\text{---} \\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "\\text{---} & \\frac{\\textbf{x}_n}{\\sqrt{m}} & \\text{---} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\vert & \\vert & \\ldots & \\vert \\\\\n",
    "\\frac{\\textbf{x}_1}{\\sqrt{m}} & \\frac{\\textbf{x}_2}{\\sqrt{m}} & \\ldots & \\frac{\\textbf{x}_n}{\\sqrt{m}}\\\\\n",
    "\\vert & \\vert & \\ldots & \\vert \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\textbf{x}_1^T \\textbf{x}_1}{m} & \\frac{\\textbf{x}_1^T \\textbf{x}_2}{m} & \\ldots & \\frac{\\textbf{x}_1^T \\textbf{x}_n}{m} \\\\\n",
    "\\frac{\\textbf{x}_2^T \\textbf{x}_1}{m} & \\frac{\\textbf{x}_2^T \\textbf{x}_2}{m} & \\ldots & \\frac{\\textbf{x}_2^T \\textbf{x}_n}{m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\textbf{x}_n^T \\textbf{x}_1}{m} & \\frac{\\textbf{x}_n^T \\textbf{x}_2}{m} & \\ldots & \\frac{\\textbf{x}_n^T \\textbf{x}_n}{m} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Since\n",
    "\n",
    "$$\n",
    "\\frac{1}{m} \\textbf{x}_{i}^T \\textbf{x}_{j} = \\frac{1}{m} \\sum_{k=0}^m x_{i}[k] x_{j}[k] = R_{X_i X_j}\n",
    "$$\n",
    "\n",
    "for random processes $X_i$, $X_j$ of constant joint density function, that is, realizations have equal probabilities, and since we processed $x_i$ and $x_j$ to have zero mean, that is\n",
    "\n",
    "$$\n",
    "\\bar{X}_i = 0, \\bar{X}_j = 0\n",
    "$$\n",
    "\n",
    "we have that\n",
    "\n",
    "$$\n",
    "C_{X_i X_j} = R_{X_i X_j}\n",
    "$$\n",
    "\n",
    "and therefore, matrix $\\textbf{X}^T\\textbf{X}$ is the covariant matrix $\\textbf{C}$ of random variables. The eigenvectors of $\\textbf{X}^T\\textbf{X}$ are such that, for any vector $\\textbf{v}$,\n",
    "\n",
    "$$\n",
    "\\textbf{X}^T\\textbf{X} \\textbf{v} = \\lambda \\textbf{v} \\rightarrow |\\textbf{X}^T\\textbf{X} - \\lambda \\textbf{I}| = 0\n",
    "$$\n",
    "\n",
    "Intuitively, eigenvector $\\textbf{v}_i$ associated with the largest eigenvalue $\\lambda_i$ is the direction of maximum variance of the data matrix. Since matrix $\\Sigma$ is composed of the eigenvalues of the covariance matrix $\\textbf{X}^T\\textbf{X}$, one can use the $k$ first singular-values of $\\Sigma$ to construct an optimal approximation of $\\textbf{X}$ that preseves its maximum variance directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: tensor([[  9.7085,  -6.1376],\n",
      "        [ 24.3131,   8.0616],\n",
      "        [-16.6365,  -8.1469],\n",
      "        [ -3.9353,  11.2802],\n",
      "        [-12.1258, -24.1382],\n",
      "        [ -9.4787,  11.9260],\n",
      "        [ -0.1671, -30.8716],\n",
      "        [ -6.1423,  -9.2955],\n",
      "        [-12.7539,  14.5391],\n",
      "        [ -9.1334, -28.7375],\n",
      "        [ 12.5862, -32.3506],\n",
      "        [ -6.2338,  22.5897],\n",
      "        [ 24.3726,   3.9999],\n",
      "        [ -7.3549,  10.1281],\n",
      "        [ 10.5294,   4.4270],\n",
      "        [  1.6304,  17.5724],\n",
      "        [-15.5117,   7.6396],\n",
      "        [-19.1626,   9.2377],\n",
      "        [ 12.1654,  30.2859],\n",
      "        [ 23.3303, -12.0092]]) \n",
      "\n",
      "XtX: tensor([[3757.3967, -110.4823],\n",
      "        [-110.4823, 6306.8232]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "# 20 observations of 2 random variables x1,x2\n",
    "# x1 = 5*noise\n",
    "# x2 = 10*noise\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "n_observations = 20\n",
    "\n",
    "# Original values - The more observations we have, the more we can expect random variables x1 and x2 to be uncorelated.\n",
    "x1=200*(torch.rand(n_observations)-0.5)     # variable x1 is uniformely distributed between [-100, 100]\n",
    "x2=300*(torch.rand(n_observations))     # variable x2 is uniformely distributed between [0, 300]\n",
    "\n",
    "# Normalize mean\n",
    "x1=x1-x1.mean()\n",
    "x2=x2-x2.mean()\n",
    "\n",
    "# Divide by sqrt(n_observations)\n",
    "x1=torch.div(x1, math.sqrt(n_observations))\n",
    "x2=torch.div(x2, math.sqrt(n_observations))\n",
    "\n",
    "# Data matrix X\n",
    "X=torch.stack((x1,x2),dim=1)\n",
    "\n",
    "print(\"X:\", X, \"\\n\")\n",
    "\n",
    "# Covariance matrix X^T*X\n",
    "XtX = torch.matmul(torch.transpose(X, 0,1),X)\n",
    "\n",
    "print(\"XtX:\", XtX, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U: tensor([[ 0.0825,  0.1540],\n",
      "        [-0.0882,  0.4022],\n",
      "        [ 0.0934, -0.2771],\n",
      "        [-0.1440, -0.0562],\n",
      "        [ 0.2970, -0.2148],\n",
      "        [-0.1551, -0.1462],\n",
      "        [ 0.3881, -0.0245],\n",
      "        [ 0.1136, -0.1067],\n",
      "        [-0.1898, -0.1977],\n",
      "        [ 0.3564, -0.1692],\n",
      "        [ 0.4137,  0.1824],\n",
      "        [-0.2875, -0.0857],\n",
      "        [-0.0370,  0.4003],\n",
      "        [-0.1314, -0.1128],\n",
      "        [-0.0499,  0.1748],\n",
      "        [-0.2201,  0.0390],\n",
      "        [-0.1045, -0.2476],\n",
      "        [-0.1266, -0.3060],\n",
      "        [-0.3742,  0.2198],\n",
      "        [ 0.1637,  0.3720]]) \n",
      "\n",
      "S: tensor([79.4456, 61.2586]) \n",
      "\n",
      "V: tensor([[ 0.0432,  0.9991],\n",
      "        [-0.9991,  0.0432]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate SVD\n",
    "U,S,V = torch.svd(X)\n",
    "\n",
    "print(\"U:\",U,\"\\n\")\n",
    "print(\"S:\",S,\"\\n\")\n",
    "print(\"V:\",V,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "*Probability, Random Variables and Random Signal Proncipals - Peyton Peebles* (ASIN:0070474281;ISBN-10:9780070474284;ISBN-13:978-0070474284)\n",
    "\n",
    "*Covariance and correlation matrix. Sums of random variables, their distributions, characteristics and characteristic functions.*\n",
    "https://web.sgh.waw.pl/~rlocho/classes_5_PTSP2019.pdf\n",
    "\n",
    "*Principal component analysis - Herve Abdi and Lynne J. Williams* (https://wires.onlinelibrary.wiley.com/doi/pdfdirect/10.1002/wics.101?download=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "016cd2178a8053b5595958e59a0d24370f913bb5fc4da5760f6ef0caa289bc0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
