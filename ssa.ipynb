{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of SSA methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSA stands for *Singular Spectrum Analysis*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation of signal subspace\n",
    "\n",
    "First, we define a Matrix $\\textbf{H}[t] \\in \\real^{w \\times M}$ by\n",
    "\n",
    "$$\n",
    "\\textbf{H}^{w \\times M}[t] = \\begin{bmatrix}\n",
    "h[t-w-M+2] & h[t-w-M+3] & \\ldots & h[t-w+1]\\\\\n",
    "h[t-w-M+3] & h[t-w-M+4] & \\ldots & h[t-w+2]\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "h[t-M+1] & h[t-M+2] & \\ldots & h[t]\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We can understand this matrix as having $w$ slices of $M$ causal-samples of the time series in its rows, as in\n",
    "\n",
    "$$\n",
    "\\textbf{H}^{w \\times M}[t] = \\begin{bmatrix}\n",
    "\\text{---} & \\textbf{h}[t-(w-1)] & \\text{---}\\\\\n",
    "\\text{---} & \\textbf{h}[t-(w-2)] & \\text{---}\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "\\text{---} & \\textbf{h}[t] & \\text{---}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For example, for $w=3$ and $M=3$:\n",
    "\n",
    "$$\n",
    "\\textbf{H}^{3 \\times 3}[t] = \\begin{bmatrix}\n",
    "h[t-4] & h[t-3] & h[t-2]\\\\\n",
    "h[t-3] & h[t-2] & h[t-1]\\\\\n",
    "h[t-2] & h[t-1] & h[t]\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We notice that $\\textbf{H}$ is a strictly causal. In case $w = M$, $\\textbf{H}$ is a simmetric square matrix.\n",
    "\n",
    "The SSA algorithm is defined by obtaining the principal components of $h[t]$ by solving the following eigenvalue problem:\n",
    "\n",
    "$$\n",
    "(\\textbf{H} \\textbf{H}^T \\boldsymbol{\\Phi})[t] = (\\boldsymbol{\\Phi} \\boldsymbol{\\Sigma})[t]\n",
    "$$\n",
    "\n",
    "Matrix $\\textbf{H} \\textbf{H}^T \\in \\real^{w \\times M}$ is equal to\n",
    "\n",
    "$$\n",
    "\\textbf{H} \\textbf{H}^T =\n",
    "\\begin{bmatrix}\n",
    "\\text{---} & \\textbf{h}[t-(w-1)] & \\text{---}\\\\\n",
    "\\text{---} & \\textbf{h}[t-(w-2)] & \\text{---}\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "\\text{---} & \\textbf{h}[t] & \\text{---}\\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "\\vert & \\vert & \\ldots & \\vert\\\\\n",
    "\\textbf{h}[t-(w-1)] & \\textbf{h}[t-(w-2)] & \\ldots & \\textbf{h}[t]\\\\\n",
    "\\vert & \\vert & \\ldots & \\vert\\\\\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "= \n",
    "\\\\\n",
    "\\begin{bmatrix}\n",
    "\\textbf{h}[t-(w-1)] \\textbf{h}[t-(w-1)] & \\textbf{h}[t-(w-1)] \\textbf{h}[t-(w-2)] & \\ldots & \\textbf{h}[t-(w-1)] h[t]\\\\\n",
    "\\textbf{h}[t-(w-2)] \\textbf{h}[t-(w-1)] & \\textbf{h}[t-(w-2)] \\textbf{h}[t-(w-2)] & \\ldots & \\textbf{h}[t-(w-2)] h[t]\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\textbf{h}[t] \\textbf{h}[t-(w-1)] & \\textbf{h}[t] \\textbf{h}[t-(w-2)] & \\ldots & \\textbf{h}[t] \\textbf{h}[t]\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If we define the auto-correlation function $R_{XX}(t,\\tau)$ for a time series $h[t] \\in \\real$ as\n",
    "\n",
    "$$\n",
    "R_{XX}(t,\\tau) = \\textbf{h}_t \\cdot \\textbf{h}_{t-\\tau} = \\sum_{i=0}^{M} h_t[i] h_{t-\\tau}[i]\n",
    "$$\n",
    "\n",
    "it is easy to see that matrix $\\textbf{H} \\textbf{H}^T$ is an autocorrelation matrix of signal $h[t]$,\n",
    "\n",
    "$$\n",
    "\\textbf{H} \\textbf{H}^T =\n",
    "\\begin{bmatrix}\n",
    "R_{XX}(t, 0) & R_{XX}(t, 1) & \\ldots & R_{XX}(t, w-1)\\\\\n",
    "R_{XX}(t, 1) & R_{XX}(t, 0) & \\ldots & R_{XX}(t, w-2)\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "R_{XX}(t, w-1) & R_{XX}(t, w-2) & \\ldots & R_{XX}(t, 0)\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If signal $h[t]$ is a wide-sense stationary process, we should expect the autocorrelation function to be only dependent on variable $\\tau$, and therefore\n",
    "\n",
    "$$\n",
    "\\textbf{H} \\textbf{H}^T =\n",
    "\\begin{bmatrix}\n",
    "R_{XX}(0) & R_{XX}(1) & \\ldots & R_{XX}(w-1)\\\\\n",
    "R_{XX}(1) & R_{XX}(0) & \\ldots & R_{XX}(w-2)\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "R_{XX}(w-1) & R_{XX}(w-2) & \\ldots & R_{XX}(0)\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The autocorrelation matrix is a positive-semidefinite, hermitian matrix. It is also a circulant matrix, where input $r_{ij} = r_{ji}$. It is known that the eigenvalues of a circulant matrix coincide with the discrete Fourier transform over one period, and the complete set of eigenvalues can be obtained by taking the DFT of any row. Since the power spectral density of a signal is related to the autocorrelation by the Fourier transform, the distribution of eigenvalues of the autocorrelation matrix approach the power spectrum density asymptotically as the order of the matrix increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Change Point Detection by Monitoring Difference Subspace between Signal Subspaces\n",
    "\n",
    "[Autocorrelation matrix eigenvalues and the power spectrum](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2006/EECS-2006-90.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "016cd2178a8053b5595958e59a0d24370f913bb5fc4da5760f6ef0caa289bc0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
